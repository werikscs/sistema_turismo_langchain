{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0fe2034",
   "metadata": {},
   "source": [
    "# Projeto de Turismo com LangChain - Passo 3: Criação e Configuração do Router\n",
    "\n",
    "Este é o notebook final do nosso fluxo de desenvolvimento. Aqui, vamos unir todas as cadeias especializadas que construímos no passo anterior sob um único orquestrador: a `RouterChain`.\n",
    "\n",
    "O objetivo é criar um sistema que:\n",
    "1.  Recebe uma pergunta do usuário em linguagem natural.\n",
    "2.  Usa um LLM para classificar a **intenção** da pergunta.\n",
    "3.  Direciona a pergunta para a cadeia especialista mais apropriada.\n",
    "4.  Executa a cadeia escolhida e retorna a resposta final.\n",
    "\n",
    "Este componente é o que torna nosso sistema inteligente e flexível."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cfc344",
   "metadata": {},
   "source": [
    "## 0. Configuração Inicial e Recriação das Cadeias\n",
    "\n",
    "Como nos notebooks anteriores, começamos inicializando nossos componentes principais (LLM, Embeddings, Pinecone).\n",
    "\n",
    "Para manter este notebook autocontido e organizado, vamos redefinir nossas cadeias especializadas dentro de funções. Isso torna o código mais limpo e fácil de gerenciar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340775ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Componentes do LangChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Componentes do Router\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "# Carrega as variáveis do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Pega as variáveis do ambiente\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "print(\"--- Inicializando Componentes ---\")\n",
    "\n",
    "# 1. LLM da Groq\n",
    "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0, api_key=groq_api_key)\n",
    "\n",
    "# 2. Modelo de Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 3. Conexão com a base de dados vetorial (Pinecone)\n",
    "vectorstore = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"LLM, Embeddings e VectorStore inicializados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbaf59",
   "metadata": {},
   "source": [
    "### Funções para Criar as Cadeias Especializadas\n",
    "\n",
    "Aqui, encapsulamos a lógica de cada cadeia. Note que as cadeias de RAG (`info_local` e `logistica`) não são `LLMChain` diretas, pois elas precisam primeiro buscar dados no `vectorstore`. Para simplificar a integração com o `MultiPromptChain`, vamos criar uma estrutura que realiza a busca e depois chama a `LLMChain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b472187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates para cada cadeia\n",
    "LOCAL_INFO_TEMPLATE = \"\"\"Você é um assistente de turismo. Responda à pergunta do usuário de forma clara, usando APENAS as informações do contexto. Se a resposta não estiver no contexto, diga 'Não encontrei essa informação.'.\n",
    "Contexto: {context}\n",
    "Pergunta: {input}\n",
    "Resposta:\"\"\"\n",
    "\n",
    "LOGISTICS_TEMPLATE = \"\"\"Você é um especialista em logística de viagens. Responda à pergunta do usuário sobre transporte de forma direta, usando as informações do contexto.\n",
    "Contexto: {context}\n",
    "Pergunta: {input}\n",
    "Instruções de Transporte:\"\"\"\n",
    "\n",
    "ITINERARY_TEMPLATE = \"\"\"Você é um agente de viagens experiente. Crie um roteiro de viagem personalizado com base na solicitação do usuário. Use o contexto para sugerir pontos turísticos.\n",
    "Contexto: {context}\n",
    "Solicitação: {input}\n",
    "Roteiro Detalhado:\"\"\"\n",
    "\n",
    "TRANSLATION_TEMPLATE = \"\"\"Você é um guia de idiomas para viajantes. Responda à solicitação do usuário fornecendo frases úteis e suas traduções.\n",
    "Solicitação do Usuário: {input}\n",
    "Frases e Traduções:\"\"\"\n",
    "\n",
    "# Dicionário com os templates\n",
    "prompt_infos = {\n",
    "    \"info_local\": {\"template\": LOCAL_INFO_TEMPLATE, \"description\": \"Ideal para responder perguntas sobre um local específico, como horário de funcionamento, localização ou detalhes de um ponto turístico.\"},\n",
    "    \"logistica\": {\"template\": LOGISTICS_TEMPLATE, \"description\": \"Use para responder perguntas sobre transporte, como chegar a um lugar, aeroportos e locomoção na cidade.\"},\n",
    "    \"roteiro_viagem\": {\"template\": ITINERARY_TEMPLATE, \"description\": \"Perfeito para quando o usuário pede um roteiro ou itinerário de viagem para uma cidade por um ou mais dias.\"},\n",
    "    \"traducao\": {\"template\": TRANSLATION_TEMPLATE, \"description\": \"Útil para fornecer traduções ou frases comuns em um idioma para viajantes.\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd34d6",
   "metadata": {},
   "source": [
    "## 1. Construção do Roteador\n",
    "\n",
    "O `LLMRouterChain` é o cérebro que decide qual caminho seguir. Ele usa um prompt especial que contém as descrições de cada cadeia especialista. A qualidade dessas descrições é o fator **mais importante** para o sucesso do roteador.\n",
    "\n",
    "Depois, a `MultiPromptChain` junta tudo, atuando como o orquestrador geral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b243ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Criar as cadeias de destino (destination chains)\n",
    "destination_chains = {}\n",
    "\n",
    "for name, info in prompt_infos.items():\n",
    "    # Define as variáveis de input corretas para cada template\n",
    "    if name == \"traducao\":\n",
    "        input_vars = ['input']\n",
    "    else:\n",
    "        input_vars = ['input', 'context']\n",
    "        \n",
    "    prompt = PromptTemplate(template=info['template'], input_variables=input_vars)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "print(f\"{len(destination_chains)} cadeias de destino criadas: {list(destination_chains.keys())}\")\n",
    "\n",
    "# 2. Criar as informações para o roteador (nome, descrição)\n",
    "destinations = [f\"{name}: {info['description']}\" for name, info in prompt_infos.items()]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "# 3. Criar o prompt do roteador\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "# 4. Criar a cadeia do roteador (o cérebro)\n",
    "# Note que não criaremos mais a MultiPromptChain, apenas o roteador que decide.\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt, verbose=True)\n",
    "\n",
    "print(\"\\nRouterChain configurada com sucesso!\")\n",
    "print(\"A MultiPromptChain não será mais usada. A lógica de execução estará na função run_query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ad84b",
   "metadata": {},
   "source": [
    "## 2. Criando a Função de Execução com RAG\n",
    "\n",
    "Agora, criamos uma função que une a lógica de RAG com o roteador. Esta função irá:\n",
    "1. Receber a pergunta do usuário.\n",
    "2. Usar o `vectorstore` para buscar o contexto relevante (RAG).\n",
    "3. Chamar a `MultiPromptChain` com a pergunta **e** o contexto.\n",
    "\n",
    "Isso garante que as cadeias que precisam de contexto (`info_local`, `logistica`, `roteiro_viagem`) o recebam. A cadeia `traducao`, que não usa contexto, simplesmente o ignorará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46428c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query: str):\n",
    "    \"\"\"\n",
    "    Executa uma consulta completa:\n",
    "    1. Busca o contexto no Pinecone.\n",
    "    2. Usa o router_chain para decidir qual cadeia usar.\n",
    "    3. Executa a cadeia escolhida com o input e o contexto corretos.\n",
    "    \"\"\"\n",
    "    print(\"--- Nova Consulta ---\")\n",
    "    print(f\"Pergunta: {query}\")\n",
    "    \n",
    "    # Passo 1: Buscar contexto relevante no Pinecone\n",
    "    print(\"\\nBuscando contexto no Pinecone...\")\n",
    "    docs = vectorstore.similarity_search(query, k=4)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Passo 2: Usar o roteador para obter o nome da cadeia de destino\n",
    "    print(\"\\nUsando o roteador para decidir a cadeia...\")\n",
    "    route = router_chain.invoke({\"input\": query})\n",
    "    destination_name = route['destination']\n",
    "    \n",
    "    # É importante verificar se o destino existe\n",
    "    if destination_name in destination_chains:\n",
    "        print(f\"Rota escolhida: '{destination_name}'\")\n",
    "        \n",
    "        # Passo 3: Selecionar e executar a cadeia de destino\n",
    "        destination_chain = destination_chains[destination_name]\n",
    "        \n",
    "        # Prepara os inputs para a cadeia final\n",
    "        inputs = {\"input\": query}\n",
    "        # Adiciona o contexto apenas se a cadeia não for a de tradução\n",
    "        if destination_name != \"traducao\":\n",
    "            inputs[\"context\"] = context\n",
    "            \n",
    "        print(\"\\nExecutando a cadeia de destino...\")\n",
    "        result = destination_chain.invoke(inputs)\n",
    "        return result['text']\n",
    "    else:\n",
    "        print(\"Erro: O roteador não conseguiu encontrar um destino válido.\")\n",
    "        return \"Desculpe, não consegui processar sua pergunta.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d527c3",
   "metadata": {},
   "source": [
    "## 3. Testes Finais\n",
    "\n",
    "Vamos testar o sistema completo com diferentes tipos de perguntas para ver se o roteador escolhe a cadeia correta em cada caso. Observe os logs (`verbose=True`) para ver a decisão do `LLMRouterChain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ce50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste 1: Informação Local\n",
    "query_info_local = \"Qual o horário de funcionamento do Museu do Louvre?\"\n",
    "response = run_query(query_info_local)\n",
    "print(\"\\nResposta Final:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste 2: Logística\n",
    "query_logistica = \"Como faço para ir do aeroporto Charles de Gaulle para o centro de Paris?\"\n",
    "response = run_query(query_logistica)\n",
    "print(\"\\nResposta Final:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf1905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste 3: Roteiro de Viagem\n",
    "query_roteiro = \"Me dê um roteiro de 3 dias para o Rio de Janeiro, com foco em praias e vida noturna.\"\n",
    "response = run_query(query_roteiro)\n",
    "print(\"\\nResposta Final:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37314a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste 4: Tradução (não usará o contexto)\n",
    "query_traducao = \"Como se diz 'Onde fica o banheiro?' em francês?\"\n",
    "response = run_query(query_traducao)\n",
    "print(\"\\nResposta Final:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ae40a",
   "metadata": {},
   "source": [
    "## Conclusão do Projeto\n",
    "\n",
    "Parabéns! Você construiu com sucesso um sistema de perguntas e respostas para turistas, utilizando uma arquitetura modular e poderosa com LangChain.\n",
    "\n",
    "- **`01_Ingestao_Dados.ipynb`**: Você criou e populou sua base de conhecimento vetorial no Pinecone.\n",
    "- **`02_Construcao_Chains.ipynb`**: Você desenvolveu e testou cadeias especialistas para tarefas específicas.\n",
    "- **`03_Router_Chain.ipynb`**: Você orquestrou tudo com um roteador inteligente, que direciona as perguntas do usuário para o especialista correto, enriquecendo as respostas com informações atualizadas via RAG.\n",
    "\n",
    "Este projeto é uma excelente base que pode ser expandida com novas cadeias, mais fontes de dados e até uma interface de usuário."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sistema_turismo_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
