{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d3a7f7",
   "metadata": {},
   "source": [
    "# Projeto de Turismo com LangChain - Passo 2: Construção das Cadeias Especializadas\n",
    "\n",
    "Neste notebook, vamos criar cada uma das nossas cadeias (Chains) especializadas. Cada cadeia será um \"especialista\" em um tipo de tarefa:\n",
    "1.  **Local Info Chain**: Fornece informações factuais sobre locais.\n",
    "2.  **Logistics Chain**: Responde a perguntas sobre transporte e logística.\n",
    "3.  **Itinerary Chain**: Cria roteiros de viagem personalizados.\n",
    "4.  **Translation Chain**: Ajuda com frases úteis e traduções.\n",
    "\n",
    "O objetivo é ter componentes modulares e testados antes de uni-los com um roteador no próximo passo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063c9a3",
   "metadata": {},
   "source": [
    "## 0. Configuração Inicial\n",
    "\n",
    "Primeiro, vamos importar as bibliotecas, carregar as variáveis de ambiente e inicializar os componentes que serão compartilhados por todas as cadeias:\n",
    "- O **LLM (Groq)**, que será o cérebro de todas as cadeias.\n",
    "- O **Modelo de Embeddings** e a conexão com o **Pinecone (VectorStore)**, que serão a base para nossas cadeias de RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e11472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weriks/anaconda3/envs/sistema_turismo_langchain/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variáveis de ambiente carregadas com sucesso!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weriks/anaconda3/envs/sistema_turismo_langchain/lib/python3.13/site-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Carrega as variáveis do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Pega as variáveis do ambiente\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "# Validação das chaves\n",
    "if not all([groq_api_key, pinecone_api_key, index_name]):\n",
    "    print(\"Erro: Verifique se as variáveis GROQ_API_KEY, PINECONE_API_KEY e PINECONE_INDEX_NAME estão no arquivo .env\")\n",
    "else:\n",
    "    print(\"Variáveis de ambiente carregadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac9a681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM (Groq) inicializado.\n",
      "Modelo de Embeddings inicializado.\n",
      "Conectado ao índice 'sistema-turismo' no Pinecone.\n"
     ]
    }
   ],
   "source": [
    "# Inicialização dos componentes principais\n",
    "\n",
    "# 1. LLM da Groq (Llama3)\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.7, # Um pouco de criatividade, mas sem exageros\n",
    "    api_key=groq_api_key\n",
    ")\n",
    "print(\"LLM (Groq) inicializado.\")\n",
    "\n",
    "# 2. Modelo de Embeddings (o mesmo usado na ingestão)\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\" \n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "print(\"Modelo de Embeddings inicializado.\")\n",
    "\n",
    "# 3. Conexão com a base de dados vetorial (Pinecone)\n",
    "# Usamos `from_existing_index` para nos conectarmos ao índice que já populamos\n",
    "vectorstore = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "print(f\"Conectado ao índice '{index_name}' no Pinecone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b80d0",
   "metadata": {},
   "source": [
    "## Cadeia 1: Informações Locais (`LocalInfoChain`)\n",
    "\n",
    "**Objetivo**: Responder a perguntas diretas sobre um ponto turístico, como horários, localização, etc.\n",
    "\n",
    "**Mecanismo**: Esta cadeia usará RAG. Ela recebe um `contexto` (buscado do Pinecone) e a `pergunta` original do usuário. O prompt instrui o LLM a responder **com base estrita no contexto fornecido**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38852966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LocalInfoChain criada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Criação do Prompt e da Cadeia de Informações Locais\n",
    "\n",
    "local_info_prompt_template = \"\"\"\n",
    "Você é um assistente de turismo prestativo. Responda à pergunta do usuário de forma clara e concisa, utilizando APENAS as informações do contexto abaixo.\n",
    "Se a resposta não estiver no contexto, diga \"Desculpe, não encontrei essa informação na minha base de dados.\"\n",
    "\n",
    "**Contexto:**\n",
    "{context}\n",
    "\n",
    "**Pergunta:**\n",
    "{question}\n",
    "\n",
    "**Resposta:**\n",
    "\"\"\"\n",
    "\n",
    "local_info_prompt = PromptTemplate(\n",
    "    template=local_info_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "local_info_chain = LLMChain(llm=llm, prompt=local_info_prompt, verbose=False)\n",
    "\n",
    "print(\"LocalInfoChain criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab329ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta de teste: Qual o horário de funcionamento do Cristo Redentor?\n",
      "\n",
      "Resposta da Cadeia:\n",
      "Geralmente das 8h às 19h, mas pode variar. É essencial verificar o site oficial para horários e compra de ingressos antecipados para evitar filas.\n"
     ]
    }
   ],
   "source": [
    "# Teste da LocalInfoChain\n",
    "\n",
    "question_local = \"Qual o horário de funcionamento do Cristo Redentor?\"\n",
    "print(f\"Pergunta de teste: {question_local}\")\n",
    "\n",
    "# 1. Buscar contexto relevante no Pinecone\n",
    "docs = vectorstore.similarity_search(question_local, k=3) # Busca os 3 chunks mais relevantes\n",
    "context_local = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# 2. Invocar a cadeia com o contexto e a pergunta\n",
    "response = local_info_chain.invoke({\n",
    "    \"context\": context_local,\n",
    "    \"question\": question_local\n",
    "})\n",
    "\n",
    "print(\"\\nResposta da Cadeia:\")\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89b07a",
   "metadata": {},
   "source": [
    "## Cadeia 2: Logística de Transporte (`LogisticsChain`)\n",
    "\n",
    "**Objetivo**: Responder perguntas de \"como chegar\", meios de transporte, etc.\n",
    "\n",
    "**Mecanismo**: Similar à `LocalInfoChain`, utiliza RAG com um prompt focado em fornecer instruções claras de logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d204d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticsChain criada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Criação do Prompt e da Cadeia de Logística\n",
    "\n",
    "logistics_prompt_template = \"\"\"\n",
    "Você é um especialista em logística de viagens. Responda à pergunta do usuário sobre transporte de forma direta e fácil de entender, usando as informações do contexto.\n",
    "\n",
    "**Contexto:**\n",
    "{context}\n",
    "\n",
    "**Pergunta:**\n",
    "{question}\n",
    "\n",
    "**Instruções de Transporte:**\n",
    "\"\"\"\n",
    "\n",
    "logistics_prompt = PromptTemplate(\n",
    "    template=logistics_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "logistics_chain = LLMChain(llm=llm, prompt=logistics_prompt, verbose=False)\n",
    "\n",
    "print(\"LogisticsChain criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab3feae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta de teste: Como eu chego na Torre Eiffel usando o metrô?\n",
      "\n",
      "Resposta da Cadeia:\n",
      "Para chegar à Torre Eiffel usando o metrô, você precisará seguir os seguintes passos:\n",
      "\n",
      "1. **Metrô:** Pegue a linha 6 do metrô e desça na estação **Bir-Hakeim**. Essa estação fica bem próxima da Torre Eiffel e oferece uma vista impressionante do monumento.\n",
      "2. **Saindo do Metrô:** Saindo da estação, você será direcionado a um ponto de vistas da Torre Eiffel. Se quiser uma vista melhor, pode seguir em direção à estação **Trocadéro**, que fica mais próxima da Torre Eiffel e oferece uma vista incrível do monumento e do rio Sena.\n",
      "3. **Caminhada:** A partir dessa estação, você precisará caminhar alguns minutos até a Torre Eiffel. A caminhada é agradável e oferece uma visão da cidade.\n",
      "\n",
      "Lembre-se de que é recomendável comprar ingressos online com antecedência para evitar filas enormes e aproveitar ao máximo sua visita à Torre Eiffel.\n"
     ]
    }
   ],
   "source": [
    "# Teste da LogisticsChain\n",
    "\n",
    "question_logistics = \"Como eu chego na Torre Eiffel usando o metrô?\"\n",
    "print(f\"Pergunta de teste: {question_logistics}\")\n",
    "\n",
    "# 1. Buscar contexto\n",
    "docs = vectorstore.similarity_search(question_logistics, k=3)\n",
    "context_logistics = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# 2. Invocar a cadeia\n",
    "response = logistics_chain.invoke({\n",
    "    \"context\": context_logistics,\n",
    "    \"question\": question_logistics\n",
    "})\n",
    "\n",
    "print(\"\\nResposta da Cadeia:\")\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859e4d6",
   "metadata": {},
   "source": [
    "## Cadeia 3: Roteiros de Viagem (`ItineraryChain`)\n",
    "\n",
    "**Objetivo**: Criar um roteiro de viagem com base nas preferências do usuário.\n",
    "\n",
    "**Mecanismo**: Esta é a nossa cadeia mais criativa. Ela ainda usa RAG para obter informações sobre as atrações da cidade, mas o prompt é mais elaborado, pedindo ao LLM para agir como um agente de viagens e montar um plano detalhado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965016b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItineraryChain criada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Criação do Prompt e da Cadeia de Roteiros\n",
    "\n",
    "itinerary_prompt_template = \"\"\"\n",
    "Você é um agente de viagens experiente e criativo. Sua tarefa é criar um roteiro de viagem personalizado com base nas informações fornecidas.\n",
    "Utilize o contexto para sugerir pontos turísticos e atividades relevantes.\n",
    "\n",
    "**Contexto sobre o destino:**\n",
    "{context}\n",
    "\n",
    "**Detalhes da Viagem:**\n",
    "- **Cidade:** {city}\n",
    "- **Duração:** {days} dias\n",
    "- **Perfil da Viagem:** {profile}\n",
    "\n",
    "**Roteiro Detalhado (sugestão dia a dia):**\n",
    "\"\"\"\n",
    "\n",
    "itinerary_prompt = PromptTemplate(\n",
    "    template=itinerary_prompt_template,\n",
    "    input_variables=[\"context\", \"city\", \"days\", \"profile\"]\n",
    ")\n",
    "\n",
    "itinerary_chain = LLMChain(llm=llm, prompt=itinerary_prompt, verbose=False)\n",
    "\n",
    "print(\"ItineraryChain criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0a8f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pedido de roteiro: Focado em natureza e vistas panorâmicas para Rio de Janeiro por 2 dias.\n",
      "\n",
      "Resposta da Cadeia:\n",
      "**Roteiro Detalhado para 2 Dias no Rio de Janeiro**\n",
      "\n",
      "**Dia 1:**\n",
      "\n",
      "1. **9h00 - 10h00:** Inicie o dia visitando a Praia de Copacabana, uma das praias mais famosas do mundo. Alugue uma cadeira de praia e aproveite o sol.\n",
      "2. **10h30 - 12h30:** Dirija-se ao Morro do Corcovado, onde encontra a famosa estátua do Cristo Redentor. Pode ir de trem ou van oficial. Não recomendado ir de carro particular.\n",
      "3. **13h00 - 14h00:** Desfrute do almoço em um restaurante localizado no bairro do Lapa, conhecido por sua arquitetura colonial e vida noturna.\n",
      "4. **15h00 - 17h00:** Visite o Pão de Açúcar, um morro com vista panorâmica para a cidade e a Baía de Guanabara.\n",
      "5. **19h00 - 20h00:** Jante em um restaurante de alta gastronomia na Zona Sul, conhecida por sua culinária internacional.\n",
      "\n",
      "**Dia 2:**\n",
      "\n",
      "1. **9h00 - 10h00:** Visite o Parque Nacional da Tijuca, um dos maiores parques urbanos do mundo, com trilhas e vistas incríveis.\n",
      "2. **10h30 - 12h30:** Desfrute de um passeio de barco pela Baía de Guanabara, com vista para a cidade e o Pão de Açúcar.\n",
      "3. **13h00 - 14h00:** Almoce em um restaurante localizado na Praia de Ipanema, uma das praias mais famosas do mundo.\n",
      "4. **15h00 - 17h00:** Visite a praia de Leblon, conhecida por suas águas cristalinas e seu ambiente tranquilo.\n",
      "5. **19h00 - 20h00:** Jante em um restaurante de alta gastronomia na Zona Sul, conhecida por sua culinária internacional.\n",
      "\n",
      "**Dicas Extra:**\n",
      "\n",
      "- Verifique os horários e preços de ingressos para os pontos turísticos antes de sair.\n",
      "- Use aplicativos de transporte público para planejar suas rotas.\n",
      "- Não esqueça de levar água, protetor solar e guarda-chuva para proteger-se do sol e da chuva.\n",
      "- Experimente a culinária local, incluindo pratos típicos como feijoada e churrasco.\n",
      "\n",
      "**Previsão de Anulação de Orçamento:**\n",
      "\n",
      "- **Ingressos:** R$ 100-200 por pessoa\n",
      "- **Transporte:** R$ 20-50 por pessoa\n",
      "- **Alimentação:** R$ 100-200 por pessoa\n",
      "- **Hotel:** R$ 200-500 por noite\n",
      "\n",
      "Total estimado: R$ 1.200-2.500 por pessoa\n"
     ]
    }
   ],
   "source": [
    "# Teste da ItineraryChain\n",
    "\n",
    "# Detalhes da viagem para o teste\n",
    "city_test = \"Rio de Janeiro\"\n",
    "days_test = \"2\"\n",
    "profile_test = \"Focado em natureza e vistas panorâmicas\"\n",
    "\n",
    "print(f\"Pedido de roteiro: {profile_test} para {city_test} por {days_test} dias.\")\n",
    "\n",
    "# 1. Buscar contexto sobre a cidade\n",
    "# A busca é mais genérica para pegar várias atrações\n",
    "docs = vectorstore.similarity_search(f\"Pontos turísticos no {city_test}\", k=5) \n",
    "context_itinerary = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# 2. Invocar a cadeia\n",
    "response = itinerary_chain.invoke({\n",
    "    \"context\": context_itinerary,\n",
    "    \"city\": city_test,\n",
    "    \"days\": days_test,\n",
    "    \"profile\": profile_test\n",
    "})\n",
    "\n",
    "print(\"\\nResposta da Cadeia:\")\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119eae7",
   "metadata": {},
   "source": [
    "## Cadeia 4 (Bônus): Tradução (`TranslationChain`)\n",
    "\n",
    "**Objetivo**: Fornecer frases úteis ou traduções simples para viajantes.\n",
    "\n",
    "**Mecanismo**: Esta cadeia é diferente, pois **não usa RAG**. Ela depende apenas do conhecimento pré-treinado do LLM para realizar a tarefa. É uma `LLMChain` simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d57d4995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TranslationChain criada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Criação do Prompt e da Cadeia de Tradução\n",
    "\n",
    "translation_prompt_template = \"\"\"\n",
    "Você é um guia de idiomas para viajantes. Responda à solicitação do usuário fornecendo frases úteis e suas traduções. Seja claro e direto.\n",
    "\n",
    "**Solicitação do Usuário:**\n",
    "{request}\n",
    "\n",
    "**Frases e Traduções:**\n",
    "\"\"\"\n",
    "\n",
    "translation_prompt = PromptTemplate(\n",
    "    template=translation_prompt_template,\n",
    "    input_variables=[\"request\"]\n",
    ")\n",
    "\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt, verbose=False)\n",
    "\n",
    "print(\"TranslationChain criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adbbbbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solicitação de teste: Frases úteis para pedir um café e a conta em um restaurante em Paris.\n",
      "\n",
      "Resposta da Cadeia:\n",
      "Aqui estão algumas frases úteis para pedir um café e a conta em um restaurante em Paris:\n",
      "\n",
      "1. **Pedir um café:**\n",
      " * \"Pouvez-vous me donner un café, s'il vous plaît?\" - Pode me dar um café, por favor?\n",
      " * \"Je voudrais un café, s'il vous plaît.\" - Eu gostaria de um café, por favor.\n",
      " * \"Un café, s'il vous plaît.\" - Um café, por favor.\n",
      "\n",
      "2. **Pedir a conta:**\n",
      " * \"L'addition, s'il vous plaît.\" - A conta, por favor.\n",
      " * \"La note, s'il vous plaît.\" - A nota, por favor.\n",
      " * \"Pouvez-vous me donner l'addition, s'il vous plaît?\" - Pode me dar a conta, por favor?\n",
      "\n",
      "3. **Agradecer:**\n",
      " * \"Merci beaucoup.\" - Muito obrigado/a.\n",
      " * \"Merci.\" - Obrigado/a.\n",
      " * \"De rien.\" - De nada.\n",
      "\n",
      "4. **Despedir-se:**\n",
      " * \"Au revoir.\" - Até logo.\n",
      " * \"À bientôt.\" - Até breve.\n",
      " * \"Bonne journée.\" - Boa sorte.\n",
      "\n",
      "Lembre-se de que em Paris, é comum dizer \"s'il vous plaît\" ao final das frases para mostrar cortesia. Além disso, é sempre útil aprender um pouco de francês antes de viajar para a França. Boa sorte e aproveite seu tempo em Paris!\n"
     ]
    }
   ],
   "source": [
    "# Teste da TranslationChain\n",
    "\n",
    "request_translation = \"Frases úteis para pedir um café e a conta em um restaurante em Paris.\"\n",
    "print(f\"Solicitação de teste: {request_translation}\")\n",
    "\n",
    "# Invocar a cadeia (sem necessidade de contexto)\n",
    "response = translation_chain.invoke({\"request\": request_translation})\n",
    "\n",
    "print(\"\\nResposta da Cadeia:\")\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c1ad3",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Excelente! Agora temos quatro cadeias especializadas, cada uma com seu próprio prompt e lógica, devidamente testadas. Estamos prontos para o próximo e último passo: construir o **RouterChain**, o componente que irá analisar a pergunta do usuário e direcioná-la para o especialista correto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sistema_turismo_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
